Finished
	Make fuzz.autotools.sh more robust to handle binutils.
	Specifically, it should handle originalLDFLAGS.txt better,
	it should search for multiple targets that were generated,
	and then compile all of them in parallel.

	Make fuzz.binutils and fuzz.coreutils that download and compile both seamlessly.

	Add a FunctionCoveragePass, which does the exact same thing as BlockCoveragePass,
    except it traces just functions, not whole blocks.
    This will be useful for when the user wants to examine the output trace themselves,
    and it will help reduce the output size since there are less functions than blocks,
    especially when heavily inlined.

    Write a LEB128 output buffer class for compression.

    After testing on "nm-new.coverage `which opt-9-debug` | cxxfilt.coverage",
    which generated 24 GB of coverage output for cxxfilt along (opt-9 is 1.25 GB),
    I realized I really need to compress the output in some way,
    especially since running normal compression tools on the resulting output
    drastically compresses them:
    	blocks.bin: 400x smaller
    	nonSingle.bin: 540x smaller
    	single.bin: 4800x smaller

    For onBlock, most sequentially executed blocks are near each other,
    so if I use the difference between adjacent blocks, they'll be all mostly small.
    Therefore, I can use a varint encoding, such as signed LEB128.

    For onInfiniteBranch, there's no pattern, but varint encoding might still help.

    For onMultiBranch, the numbers are likely very low, so varint encoding will definitely help.

    Results of LEB128 encoding on "nm-new.coverage `which opt-9-debug` | cxxfilt.coverage > /dev/null":
    	blocks.bin: 7x smaller
    	nonSingle.bin: 2x smaller
    Normal compression algorithms still help a lot on top of LEB128.

	Investigate the performance degradation when adding the coverage.
	"nm-new.coverage `which opt-9-debug` | cxxfilt.coverage > /dev/null" took ~8 minutes,
	but "nm-new `which opt-9-debug` | cxxfilt > /dev/null" only took ~8 seconds.
	I'm trying to figure out how to profile it now using clang (apparently gprof doesn't work).

	Running perf on cxxfilt.coverage showed that most of the time is spent in __BranchCoverage_onMultiBranch()
	calling pwrite(), because while non-single branches are buffered,
	I flush the counts on each non-single branch, because I thought they were a lot rarer than they are.
	This just means that I shouldn't flush the counts every time,
	or maybe I should just mmap the memory for the counts so it's just a simple store instead of pwrite.

    I fixed most of the slow performance by only flushing the counts
    when the buffer is flushed also.
    Since the buffers are unconditionally flushed on program exit/crash,
    the counts will be saved always as well.

    This removes pwrite() and __BranchCoverage_onMultiBranch() as a hotspot
    in the perf flamegraph, speeding up execution of cxxfilt.coverage by ~10x.
    It's still ~10x slower than the un-instrumented cxxfilt, though.

Next
	onMultiBranch Bugfix and Compression

	Compress single.bin.

Compression
	After testing on "nm-new.coverage `which opt-9` | cxxfilt.coverage",
	which generated 24 GB of coverage output for cxxfilt along (opt-9 is 1.25 GB),
	I realized I really need to compress the output in some way,
	especially since running normal compression tools on the resulting output
	drastically compresses them:
		blocks.bin: 400x smaller
		nonSingle.bin: 540x smaller
		single.bin: 4800x smaller

	For onBlock, most sequentially executed blocks are near each other,
	so if I use the difference between adjacent blocks, they'll be all mostly small.
	Therefore, I can use a varint encoding, such as signed LEB128.

	For onInfiniteBranch, there's no pattern, but varint encoding might still help.

	For onMultiBranch, the numbers are likely very low, so varint encoding will definitely help.

	For onSingleBranch, varint encoding won't work since they're just single bits already,
	but obviously a 5000x compression is insane, so there's a huge amount of redundancy.
	I'll have to investigate the file itself to find patterns,
	but it might be that there are long strings of 0s and then long strings of 1s (maybe from loops),
	in which case I can record the value (0 or 1) and the number of repetitions
	each time they switch, which can potentially drastically compress.

	On top of this, it's also possible to run a generic compression algorithm
	like xz, bzip2, or gzip, although I'll have to be careful to make sure
	it compresses fast enough to not hold up the program.
	Single-threaded (which it will be if embedded) xz can't compress a 13 GB blocks.bin
	in any reasonable amount of time (definitely longer than how long the program took),
	so certainly for blocks.bin xz is too slow and maybe the faster gzip is better.
	There will always be more blocks than branches, since each branch is in it's own block,
	so blocks.bin will need the fastest compressor.
	Something like single.bin, though, could use a slower compressor like xz since it's smaller.

Parser
	I need to write a parser for all the coverage output generated,
	especially once I apply the compression above.
	The question, though, is how it should be designed.
	Should it be as a C++ library, maybe with CPython bindings?
	Or should it just output to plain text (although this could be huge)?

	I should also integrate the blocks source maps for blocks.bin.
	I could also make the blocks source map binary encoded,
	perhaps using varint encoding as well,
	and then embed it in the executable itself as a raw text symbol during the LLVM pass.
	Or maybe the source map should be placed in coverage.out.dir/<program name>.

	Decision:
	I should keep creating a separate text blocks source map.
	But I should also create a binary blocks source map at compile time
	that I embed in the executable itself.
	At runtime, this embedded source map is written to a file in coverage.out.dir.
	This way the source map data needed for reading the coverage data
	is always right there with the coverage data.

Others
	Make Mask constexpr, will be much faster in critical sections

Meta Compression
	Junfeng gave me a great idea about how to drastically reduce the amount of output generated.

	First, the branch coverage should be enough to fully recover the block coverage,
	and from the block coverage the function coverage can be trivially recovered.

	The branch coverage tells us all the decisions the code makes and what path it takes,
	so from that, I can easily tell the order of blocks the program goes through.
	The branch coverage output for single branches is much smaller (even uncompressed)
	than the block coverage output, so this should be a big win.
	The branch coverage output for multi branches (switch case) can be bigger,
	but I'm still working on compressing it further.

	The only place it can go wrong is on "infinite" branches,
	i.e. where the program jumps to another variable location and starts executing,
	like calling a function pointer or a virtual method call.

	It may be possible to reduce some of these to multi branches,
	such as virtual method calls (there are a limited number of classes whose virtual methods may be called),
	or function pointers that are only assigned a limited number of constant pointers.

	However, we could also bring back the block coverage just for these cases.
	In __BranchCoverage_onInfiniteBranch(), we can set a flag indicating an infinite branch has just been made.
	Then in __BlockCoverage_onBlock(), we can check this flag, and if true,
	record the current block (or delta) instead of the function address.

	Second, beyond storing single branches as repetition counts (which I'm not sure is always common),
	there's another way to drastically reduce the branch coverage output.
	Essentially, we can use profile guided optimization for this.
	On the first run, or the first few, we record the full output.
	From the output, we determine the most common branch in all cases (single, multi, infinite),
	and then when running the program again, don't record any output for the most common branch,
	and when parsing the output, interpret the absence of output
	as indication that the most common branch was taken.

	However, I'm not totally sure if this will work, at least for single branches.
	A single branch can be stored in a single bit, which is already really small,
	and since we don't where (which block) each branch came from, we can't tell the difference
	between a missing branch and just the next branch.
	If repetition counts are worth it, then single branches will be compressed even more,
	so I don't think this should be that much of an issue.

	This could work, however, for non-single branches, where there are a lot more possibilities in general.
	I think we still need to save some output for the most common branch
	since we still don't know where the branch is.
	But these are variable (LEB128 varint) outputs, so we can select the smallest for the most common output.
	For example, we could store 0 for the most common branch,
	make the delta unsigned (store sign bit as LSB), and then add 1 to each delta.

onMultiBranch Bugfix and Compression
	I noticed that nonSingle.bin was getting very large,
    even bigger than blocks.bin on many occasions.
    The number of switch cases is generally small, so this seemed odd.
    After collecting averages of LEB128 bytes used for each non-single branch part, and averages of the actual non-single branch parts,
    I think I discovered the reason.

    While LEB128 sizes still averaged ~1 byte,
    the combined (1 bit + single branch bit index diff) averaged only ~4,
    which is actually pretty small but expected based on how many switch cases there are.
    But the low (switch value) averaged ~34 and high (num cases) averaged ~77.

    This is extremely high, but it's also a product of the specific program.
    cxxfilt uses a lot of large switch statements,
    more than most programs, especially more modern programs.
    There's not much I can do about this, however,
    since the program was just written this way.

    But in looking through the instrumented IR,
    I discovered that there are a ton of redundant onMultiBranch() calls.
    Many of the switch cases return to the same block,
    causing repeated onMultiBranch() calls for that block.

    For example, in d_count_templates_scopes(), which is one of the main demangling functions used in cxxfilt,
    there is a switch statement with 82 cases, with one of the logical branches, probably the most common,
    having 62 cases go into it (meaning each time it records 62 multi branches), and on top of that it's recursive.

    This is bad for two reasons:
    Firstly, each time on of those cases is taken,
    all of the onMultiBranch() calls are made, when only one should be.

    Secondly, this repetition of return blocks
    means that there are a lot fewer actual branches in the switch.
    When I instrument the code, I can keep track of the actual number of branches.
    This will also reduce each of the switch values and the number of cases,
    which will reduce the varint encoding output.

    Also, this is the perfect case for profile-guided optimization.
    I can sort the multi branches by probability,
    so once varint encoded, the most likely branches produce the least output.

onNonSingleBranch repeats
	In a switch statement (a multi branch), one successor block might later jump to one of the other successor blocks.
	In this case, onMultiBranch() gets called twice, but it was only supposed to be called once.
	This is a behavior unique to switch statements due to their fall-through behavior.
	The fall-through behavior allows multiple cases to go the same successor block.
	We record the switch/case value immediately before the switch instruction, like we do for br instructions,
	but then we record extra information for when multiple cases are mapped to a single successor block.
	Therefore, we want to place the onMultiBranch() call at the beginning of the successor block.
	But that block can be entered in other ways.
	We can check if this is the case by checking if the successor block has predecessor blocks other than the switch block.
	Only in this case do we need to fix the runtime behavior.

	To fix this, we add two more onMultiBranch APIs:
    	api(onSwitchCase)(bool valid, u32 branchNum, u32 numBranches) noexcept;
    Before the switch instruction, we insert a boolean local variable set to true
    to indicate that a switch instruction is going to execute.
    It's important that this variable is on the stack so that it is thread safe.
    Then onSwitchCase() is called with the boolean variable as the first argument.
    onSwitchCase() then calls onMultiBranch() if valid is true.
    The boolean variable is then set to false so that no other successor blocks erroneously call onMultiBranch().
