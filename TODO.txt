Finished
	Make fuzz.autotools.sh more robust to handle binutils.
	Specifically, it should handle originalLDFLAGS.txt better,
	it should search for multiple targets that were generated,
	and then compile all of them in parallel.

	Make fuzz.binutils and fuzz.coreutils that download and compile both seamlessly.

	Add a FunctionCoveragePass, which does the exact same thing as BlockCoveragePass,
    except it traces just functions, not whole blocks.
    This will be useful for when the user wants to examine the output trace themselves,
    and it will help reduce the output size since there are less functions than blocks,
    especially when heavily inlined.

Next
	Write a LEB128 output buffer class for compression.

Compression
	After testing on "nm-new.coverage `which opt-9` | cxxfilt.coverage",
	which generated 24 GB of coverage output for cxxfilt along (opt-9 is 1.25 GB),
	I realized I really need to compress the output in some way,
	especially since running normal compression tools on the resulting output
	drastically compresses them:
		blocks.bin: 400x smaller
		nonSingle.bin: 540x smaller
		single.bin: 4800x smaller

	For onBlock, most sequentially executed blocks are near each other,
	so if I use the difference between adjacent blocks, they'll be all mostly small.
	Therefore, I can use a varint encoding, such as signed LEB128.

	For onInfiniteBranch, there's no pattern, but varint encoding might still help.

	For onMultiBranch, the numbers are likely very low, so varint encoding will definitely help.

	For onSingleBranch, varint encoding won't work since they're just single bits already,
	but obviously a 5000x compression is insane, so there's a huge amount of redundancy.
	I'll have to investigate the file itself to find patterns,
	but it might be that there are long strings of 0s and then long strings of 1s (maybe from loops),
	in which case I can record the value (0 or 1) and the number of repetitions
	each time they switch, which can potentially drastically compress.

	On top of this, it's also possible to run a generic compression algorithm
	like xz, bzip2, or gzip, although I'll have to be careful to make sure
	it compresses fast enough to not hold up the program.
	Single-threaded (which it will be if embedded) xz can't compress a 13 GB blocks.bin
	in any reasonable amount of time (definitely longer than how long the program took),
	so certainly for blocks.bin xz is too slow and maybe the faster gzip is better.
	There will always be more blocks than branches, since each branch is in it's own block,
	so blocks.bin will need the fastest compressor.
	Something like single.bin, though, could use a slower compressor like xz since it's smaller.

Parser
	I need to write a parser for all the coverage output generated,
	especially once I apply the compression above.
	The question, though, is how it should be designed.
	Should it be as a C++ library, maybe with CPython bindings?
	Or should it just output to plain text (although this could be huge)?

	I should also integrate the blocks source maps for blocks.bin.
	I could also make the blocks source map binary encoded,
	perhaps using varint encoding as well,
	and then embed it in the executable itself as a raw text symbol during the LLVM pass.
	Or maybe the source map should be placed in coverage.out.dir/<program name>.

	Decision:
	I should keep creating a separate text blocks source map.
	But I should also create a binary blocks source map at compile time
	that I embed in the executable itself.
	At runtime, this embedded source map is written to a file in coverage.out.dir.
	This way the source map data needed for reading the coverage data
	is always right there with the coverage data.

Others
	Make Mask constexpr, will be much faster in critical sections

